<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=icon  type="image/png" sizes=152x152  href="/nlpwme/assets/robot_smaller_152x152.png"> <link rel=icon  type="image/x-icon" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/x-icon" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <link rel=icon  type="image/png" sizes=64x64  href="/nlpwme/assets/robot_smaller_64x64.png"> <link rel=icon  type="image/png" sizes=32x32  href="/nlpwme/assets/robot_smaller_32x32.png"> <title>NLPwShiyi | Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1b-info-theory">Information Theory</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1j-bayesian">Bayesian Inference</a> </div> </div> <div class=dropdown > <button class=dropbtn  >ML / DL <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/2k-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Measuring Subjectivity with VAE</a> <a href="/nlpwme/modules/4g-etl-job">Serverless ETL Example</a> <a href="/nlpwme/modules/4h-ocr-data-aug">OCR Text Augmentation</a> <a href="/nlpwme/modules/4i-neo4j-gql">Neo4j GQL Example</a> </div> </div> </nav> </nav> <script src="../assets/js/custom.js"></script> <div class=franklin-content ><p><img src="./assets/nlpwme1c.png" alt=intro  /></p> <h1 id=understanding_natural_language_processing_with_me ><a href="#understanding_natural_language_processing_with_me" class=header-anchor >Understanding Natural Language Processing with Me&#33;</a></h1> <p>Hi, I‚Äôm Shiyi. Welcome to my technical blog. Please check out this <a href="https://shiyis.github.io/nlp-docs">page</a> for a more detailed account of my journey with respect to learning Natural Language Processing. Here I will only be documenting the gists. I will be presenting everything I have learned so far, including important concepts, necessary code snippets, and more. I am by no means an expert in this subject, but I have gone through extensive studies and training in the fields and subfields related to have a good grasp of what‚Äôs important.</p> <p>Areas that I have dabbled in,</p> <pre><code class="julia hljs">‚Üí General Linguistics
‚Üí Symbolic Computational Linguistics
‚Üí Statistical Natural Language Processing
‚Üí State of the Art Large Language Modeling</code></pre> <h3 id=the_subject_matter ><a href="#the_subject_matter" class=header-anchor ><strong>The Subject Matter</strong></a></h3> <p>What do we mean by Natural Language Processing? If we do a little googling and researching, it&#39;s very intuitive that natural language processing involves a set of solutions to various natural human language tasks. The most common ones are</p> <pre><code class="plaintext hljs">‚Üí Sentiment analysis
‚Üí Machine translation
‚Üí Word-sense disambiguation
‚Üí Named-entity recognition
‚Üí Topic modeling
‚Üí Text classification
‚Üí Document Classification
‚Üí Question answering</code></pre> <h3 id=a_little_bit_of_history ><a href="#a_little_bit_of_history" class=header-anchor ><strong>A Little Bit Of History</strong></a></h3> <p>The history of Computational Linguistics dates back to the 40s to 50s. So, it&#39;s not very long ago that the field that has created ChatGPT or any form of AI that is so commonly adopted in every aspect of our lives now started to have its very first ancestral ideation. It&#39;s still a fairly new and young field with infinite possibilities up for exploration.</p> <p>Before diving in, first we have to ask ourselves what exactly is artificial intelligence &#40;AI&#41;?</p> <p>According to the official definition extracted out of John McCarthy&#39;s 2004 <a href="https://www-formal.stanford.edu/jmc/whatisai.pdf">paper</a> listed on IBM&#39;s <a href="https://www.ibm.com/topics/artificial-intelligence">website</a>,</p> <pre><code class="plaintext hljs">ü§ñÔ∏è &quot;It is the science and engineering of making intelligent machines,
 especially intelligent computer programs. It is related to the similar
 task of using computers to understand human intelligence, but AI does
 not have to confine itself to methods that are biologically observable.&quot;</code></pre> <p>So if it&#39;s to understand human intelligence, we need to know how we as humans gain information and human intelligence, or the brain, really works both through physiology and psychology,</p> <pre><code class="plaintext hljs">üí° Two Important Sources of Knowledge: Rationalism and Empiricism.

  The first acquires knowledge through reasoning and logic, while the second
  through experience and experimentation.</code></pre> <p>Below are some important notes with respect to the <strong>historical timeline</strong> of the development of Computational Linguistics and NLP and how it all started from one of these two principles and gradually transitioned to the other &#40;rationalism / computationalism to empiricism / connectionism; although computationalism is not always symbolic; namely it also incorporates empirical evidence&#41;:</p> <div class=cards ><div class=column ><div class=row ><div class=card ><div class=container ><h2> Noisy Channel Model </h2> <div class=content ><p>For the transmission of language through media like communication channels and speech acoustics, Shannon used the metaphors of the noisy channel and decoding.</p> <p>Shannon produced the first measurement of the entropy of English using probabilistic approaches, employing entropy from thermodynamics to quantify the information content of a language or the information capacity of a channel.</p> <p>Instrumental phonetics and the development of the sound spectrograph established the foundation for later work in voice recognition &#40;Koenig et al., 1946&#41;. The first automated speech recognition systems were created in the early 1950s.</p></div></div></div></div> <div class=row ><div class=card ><div class=container ><h2> Foundational Insights: 1940s - 1950s </h2> <div class=content ><p>Using Shannon&#39;s work as inspiration, Chomsky &#40;1956-7&#41; originally studied finite-state machines as a way to characterize a grammar before defining a finite-state language as a language produced by a finite-state grammar.</p> <p>These pioneering theories paved the way for formal language theory, which defines formal languages as sequences of symbols using algebra and set theory. Meanwhile, in the realm of theoretical linguistics, Chomsky &#40;1957&#41; postulated transformational and context free grammar which helped establish paradigms for describing natural language syntax.</p> <p>The work of Turing, the founding father of computer science, first resulted in the development of a computing component that was more close to propositional logic. After that, regular expressions and finite state automata were created by Stephen Kleene, inspired by the work of Emil Post in 1951 and 1956.</p> <p>Then, probabilistic models, such as Markov processes, have indeed played a significant role in automating and formalizing natural language processing. They provide a probabilistic framework for understanding language structure and have been widely used in various language processing tasks.</p></div></div></div></div> <div class=row ><div class=card ><div class=container ><h2> The Merging of Two Cultures </h2> <div class=content ><p><em>Language Theory</em>: The work of Chomsky and others on formal language theory and generative syntax throughout the late 1950s and early to mid 1960s, and work of many linguists and computational scientists on parsing algorithms, initially top-down and bottom-up and via dynamic programming.</p> <p>The Transformations and Discourse Analysis Project &#40;TDAP&#41;, developed by Zelig Harris and deployed at the University of Pennsylvania between 1958 and 1959, was one of the first full parsing systems &#40;Harris, 1962&#41;.</p> <p><em>Artificial Intelligence</em>: In the summer of 1956, researchers were called together for a two-month workshop on what John McCarthy, Marvin Minsky, Cloude Shannon, and Nathaniel Rochester came to refer to as artificial intelligence &#40;AI&#41;.</p> <p>These were simple systems that worked in single domains mainly by a combinations of pattern matching and keyboard search with simple heuristics for reasoning and question-answering. By the late 60s more formal logical systems were developed.</p></div></div></div></div> <div class=row ><div class=card ><div class=container ><h2> Paradigms Develop </h2> <div class=content ><p>The Bayesian approach was starting to be used to tackle the optical character recognition issues.</p> <p>By multiplying the likelihoods for each letter, Bledsoe and Browning &#40;1959&#41; created a Bayesian text-recognition system that made use of a large dictionary to compute the likelihood of each observed letter sequence given each word in the dictionary. Bayesian techniques were used by Mosteller and Wallace &#40;1964&#41; to address the issue of authorship attribution on the Federalist papers.</p> <p>The first online corpus was created in 1963‚Äì1964 at Brown University and contains 1 million words of samples from 500 different written texts in a variety of genres &#40;newspapers, novels, non-fiction, academic texts, etc.&#41;.</p> <p>Independently, employees at IBM and Baker at Carnegie Mellon University developed speech recognition algorithms, using techniques like the Hidden Markov Model and analogies to a noisy channel and decoding.</p></div></div></div></div> <div class=row ><div class=card ><div class=container ><h2> Empiricism Redux </h2> <div class=content ><p>After work on finite-state phonology and morphology by Kaplan and Kay &#40;1981&#41; and finite-state models of syntax by Church &#40;1980&#41;, finite-state models started to garner attention once more.</p> <p>Empiricism is making a comeback, most notably with the rise of probabilistic models used in speech and language processing, which has been greatly influenced by research at the IBM Thomas J. Watson Research Center on speech recognition probabilistic models.</p> <p>These probabilistic techniques are additional examples of data-driven strategies used in part-of-speech tagging, attachment ambiguities, parsing, and <a href="https://en.wikipedia.org/wiki/Connectionism">connectionist strategies</a> ranging from speech recognition to semantics. These were the original approaches used before the sophisticated neural language models that are used today.</p></div></div></div></div></div></div> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ¬©Ô∏è Last modified: November 01, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>